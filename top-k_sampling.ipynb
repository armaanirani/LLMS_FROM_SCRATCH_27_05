{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b49bbb31",
   "metadata": {},
   "source": [
    "### Decoding strategy 2: Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2badad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### In the previous strategy, we implemented a probabilistic sampling approach coupled with temperature scaling to increase\n",
    "### the diversity of the outputs.\n",
    "### We saw that higher temperature values result in more uniformly distributed next-token probabilities, which result in more\n",
    "### diverse outputs as it reduces the likelihood of the model repeatedly selecting the most probable token.\n",
    "### This method allows for exploring less likely but potentially more interesting and creative paths in the generation process.\n",
    "### However, one downside of this approach is that it sometimes leads to grammatically incorrect or completely nonsensical outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6504f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "vocab = {\n",
    "    \"closer\": 0,\n",
    "    \"every\": 1,\n",
    "    \"effort\": 2,\n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5,\n",
    "    \"pizza\": 6,\n",
    "    \"towards\": 7,\n",
    "    \"you\": 8\n",
    "}\n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Assume the LLM is given the start context \"every effort moves you\" and generates the following next output logits\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dc2d071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f252c248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float(\"-inf\")),\n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3819b43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd24285",
   "metadata": {},
   "source": [
    "### Merge temperature scaling and top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acca49eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We can now apply the temperature scaling and multinomial function for probabilistic sampling introduced previously to select\n",
    "### the next token among these 3 nonzero probability scores to generate the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "528f1cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: For-loop is the same as before: get logits, and only focus on last time step\n",
    "### Step 2: In this new section we filter logits with top_k sampling\n",
    "### Step 3: This is the new section where we apply temperature scaling\n",
    "### Step 4: Carry out greedy next token selection as before when temperature scaling is disabled\n",
    "### Step 5: Stop generating early if end of sequence token is encountered and eos_id is specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d203bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    \n",
    "    # For-loop is the same as before: get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "    \n",
    "        # New: filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float(\"-inf\")).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        \n",
    "        # New: apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "                \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)   # (batch_size, context_len)\n",
    "                \n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "        # Otherwise same as before: get the idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)   # (batch_size, 1)\n",
    "            \n",
    "        if idx_next == eos_id:  # Stop generating early if end of sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "        \n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1) # (batch_size, num_tokens+1)\n",
    "        \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f4e54f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
