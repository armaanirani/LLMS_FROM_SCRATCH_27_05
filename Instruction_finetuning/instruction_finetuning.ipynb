{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaab7945",
   "metadata": {},
   "source": [
    "### Step 1: Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2779d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a21945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example entry:\\n\", data[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae84cc99",
   "metadata": {},
   "source": [
    "### Converting instructions into Alpaca format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e31b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "    \n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    \n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a44ba22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2929fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5ce9c8",
   "metadata": {},
   "source": [
    "### Splitting dataset into train/test/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ff0278a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Test set length: 110\n",
      "Val set length: 55\n"
     ]
    }
   ],
   "source": [
    "train_portion = int(len(data) * 0.85)    # 85%\n",
    "test_portion = int(len(data) * 0.1) # 10%\n",
    "val_portion = len(data) - train_portion - test_portion  # 5%\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]\n",
    "\n",
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Test set length:\", len(test_data))\n",
    "print(\"Val set length:\", len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4744d75",
   "metadata": {},
   "source": [
    "### Step 2: Organizing data into training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e0b3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        \n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f0e1368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09b0253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: Find the longest sequence in the batch\n",
    "### Step 2: Pad and prepare inputs\n",
    "### Step 3: Remove extra padded token added earlier\n",
    "### Step 4: Convert list of inputs to tensor and transfer to target device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db2b3887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def custom_collate_draft_1(batch, pad_token_id=50256, device=\"cpu\"):\n",
    "    \n",
    "    # Find the longest sequence in the batch and increase the max length by +1, which will add one extra padding token below\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    \n",
    "    # Pad and prepare inputs\n",
    "    inputs_list = []\n",
    "    \n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to batch_max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # Via padded[:-1], we remove the extra padded token that has been added via the +1 setting in match_max_length\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_list.append(inputs)\n",
    "        \n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_list).to(device)\n",
    "    return inputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05cd7130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (inputs_1, inputs_2, inputs_3)\n",
    "\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866d42f3",
   "metadata": {},
   "source": [
    "### Creating target token ids for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8936ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_2(batch, pad_token_id=50256, device=\"cpu\"):\n",
    "    \n",
    "    # Find the longest sequence in the batch and increase the max length by +1, which will add one extra padding token below\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    \n",
    "    # Pad and prepare inputs\n",
    "    inputs_list, targets_list = [], []\n",
    "    \n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to batch_max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # Via padded[:-1], we remove the extra padded token that has been added via the +1 setting in match_max_length\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        # Shift +1 to the right for targets\n",
    "        targets = torch.tensor(padded[1:])\n",
    "        inputs_list.append(inputs)\n",
    "        targets_list.append(targets)\n",
    "        \n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_list).to(device)\n",
    "    targets_tensor = torch.stack(targets_list).to(device)\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26a88139",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: Truncate the last token for inputs\n",
    "### Step 2: Shift +1 to the right for targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fbdbca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (inputs_1, inputs_2, inputs_3)\n",
    "\n",
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01748bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We retain one end of ext token in the target list to allow the LLM to learn when to generate an end of text token in response to\n",
    "# instructions, which we use as an indicator that the generated response is now complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ad43110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch, pad_token_id=50256, ignore_index=-100, allowed_max_length=None, device=\"cpu\"):\n",
    "    # Find the longest sequence in the batch and increase the max length by +1, which will add one extra padding token below\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    \n",
    "    # Pad and prepare inputs\n",
    "    inputs_list, targets_list = [], []\n",
    "    \n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to batch_max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # Via padded[:-1], we remove the extra padded token that has been added via the +1 setting in match_max_length\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        # Shift +1 to the right for targets\n",
    "        targets = torch.tensor(padded[1:])\n",
    "        \n",
    "        # Replace all but the first padding tokens in the targets by ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "        \n",
    "        # Optionally truncate to maximum sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "        \n",
    "        inputs_list.append(inputs)\n",
    "        targets_list.append(targets)\n",
    "        \n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_list).to(device)\n",
    "    targets_tensor = torch.stack(targets_list).to(device)\n",
    "    \n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "833a49fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (inputs_1, inputs_2, inputs_3)\n",
    "\n",
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d5463e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Why replacing by -100?\n",
    "### For demonstration purposes, consider the following simple and self contained example where each output logit can correspond \n",
    "### to a potential token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "161bcdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n"
     ]
    }
   ],
   "source": [
    "logits_1 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "     [-0.5, 1.5]]\n",
    ")\n",
    "targets_1 = torch.tensor([0, 1])\n",
    "\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "print(loss_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8548df52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7936)\n"
     ]
    }
   ],
   "source": [
    "logits_2 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]]\n",
    ")\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(loss_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01bf1bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n",
      "loss_1 == loss_3: tensor(True)\n"
     ]
    }
   ],
   "source": [
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
    "print(loss_3)\n",
    "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19b1a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Based on this result we can see that the resulting loss on these 3 training examples is identical to the loss we calculated from\n",
    "### the 2 training examples.\n",
    "### In other words, the cross entropy loss function ignored the third entry in the targets_3 vector, the token ID corresponding\n",
    "### to -100.\n",
    "### The default setting of the cross entropy loss function has ignore_index=-100. This means that it ignores targets labeled with -100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5389d85",
   "metadata": {},
   "source": [
    "### Masking target token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "120a348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### In addition to masking out padding tokens, it is also common to mask out the target token IDs that correspond to the instruction\n",
    "### By masking out the target token IDs that correspond to the instruction, the LLM cross entropy loss is only computed for the\n",
    "### generated response target IDs.\n",
    "### By masking out the instruction tokens, the model is trained to focus on generating accurate responses rather than additionally \n",
    "### also memorizing instructions, which can help with reducing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6732b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Currently, researchers are divided on whether masking the instructions is universally beneficial during instruction finetuning\n",
    "### For instance, a recent paper titled \"Instruction tuning with loss over instructions\" demonstrated that not masking the \n",
    "### instructions benefits the LLM performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25867a43",
   "metadata": {},
   "source": [
    "### Step 3: Creating dataloaders for instruction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af3b27ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfbeec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the partial function from python functools to create a new version of the function with the device argument prefilled\n",
    "from functools import partial\n",
    "customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef0a7ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54506e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 57]) torch.Size([8, 57])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aacbebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We can see that the first input and target batch have dimensions 8x61, where 8 is the batch size and 61 is the number of \n",
    "### tokens in each training example.\n",
    "### The second input and target batch have a different number of tokens, in this case 76.\n",
    "### The custom collate function is able to create batches of different lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701b34dc",
   "metadata": {},
   "source": [
    "### Step 4: Loading a pretrained LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0a1a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the path so 'utils' can be found\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86d220b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\CS\\LLMS_FROM_SCRATCH_27_05\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up to date: gpt2\\355M\\checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\CS\\LLMS_FROM_SCRATCH_27_05\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up to date: gpt2\\355M\\encoder.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\CS\\LLMS_FROM_SCRATCH_27_05\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up to date: gpt2\\355M\\hparams.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\CS\\LLMS_FROM_SCRATCH_27_05\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up to date: gpt2\\355M\\model.ckpt.data-00000-of-00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\CS\\LLMS_FROM_SCRATCH_27_05\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up to date: gpt2\\355M\\model.ckpt.index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\CS\\LLMS_FROM_SCRATCH_27_05\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up to date: gpt2\\355M\\model.ckpt.meta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\CS\\LLMS_FROM_SCRATCH_27_05\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up to date: gpt2\\355M\\vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1024)\n",
       "  (pos_emb): Embedding(1024, 1024)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from GPTModel import GPTModel, load_weights_into_gpt\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size, models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7234fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "# Checking the pretrained LLMs performance on one of the validation tasks comparing its output to the expected response.\n",
    "torch.manual_seed(123)\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5409df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # Add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # Remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    \n",
    "    # For-loop is the same as before: get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "    \n",
    "        # New: filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float(\"-inf\")).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        \n",
    "        # New: apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "                \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)   # (batch_size, context_len)\n",
    "                \n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "        # Otherwise same as before: get the idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)   # (batch_size, 1)\n",
    "            \n",
    "        if idx_next == eos_id:  # Stop generating early if end of sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "        \n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1) # (batch_size, num_tokens+1)\n",
    "        \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26a2df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256\n",
    ")\n",
    "\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0285b5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response:\n",
      "\n",
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the\n"
     ]
    }
   ],
   "source": [
    "# we need to focus only on the model's generated response\n",
    "# We need to subtract the length of the input instruction from the start of the generated text to isolate the model's response.\n",
    "response_text = generated_text[len(input_text):].strip()\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8d51d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "### As we can see the pretrained model is not yet capable of correctly following the given instruction.\n",
    "### While it created a \"Response\" section, it simply repeats the original input sentence and part of the instruction, failing to\n",
    "### convert the active sentence to passive voice as requested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44157f0",
   "metadata": {},
   "source": [
    "### Step 5: Finetuning the LLM on instruction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11c0a36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceed the supported context size\n",
    "        # ex: if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "            \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, voacb_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "        \n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)   # (batch, 1)\n",
    "        \n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)    # (batch, n_tokens + 1)\n",
    "    \n",
    "    return idx\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, \n",
    "            idx=encoded,\n",
    "            max_new_tokens=50,\n",
    "            context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    \n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()   # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()   # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step()    # Updates model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()  # Returns the total number of elements (or tokens) in the input_batch\n",
    "            global_step += 1\n",
    "            \n",
    "            # optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "        \n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee492fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.8259098052978517\n",
      "Val loss: 3.761934757232666\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Val loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8a8dd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.637, Val loss 2.626\n",
      "Ep 1 (Step 000005): Train loss 1.174, Val loss 1.102\n",
      "Ep 1 (Step 000010): Train loss 0.872, Val loss 0.944\n",
      "Ep 1 (Step 000015): Train loss 0.857, Val loss 0.906\n",
      "Ep 1 (Step 000020): Train loss 0.776, Val loss 0.881\n",
      "Ep 1 (Step 000025): Train loss 0.754, Val loss 0.859\n",
      "Ep 1 (Step 000030): Train loss 0.799, Val loss 0.836\n",
      "Ep 1 (Step 000035): Train loss 0.714, Val loss 0.808\n",
      "Ep 1 (Step 000040): Train loss 0.672, Val loss 0.806\n",
      "Ep 1 (Step 000045): Train loss 0.633, Val loss 0.789\n",
      "Ep 1 (Step 000050): Train loss 0.663, Val loss 0.783\n",
      "Ep 1 (Step 000055): Train loss 0.760, Val loss 0.763\n",
      "Ep 1 (Step 000060): Train loss 0.719, Val loss 0.743\n",
      "Ep 1 (Step 000065): Train loss 0.653, Val loss 0.735\n",
      "Ep 1 (Step 000070): Train loss 0.532, Val loss 0.729\n",
      "Ep 1 (Step 000075): Train loss 0.569, Val loss 0.728\n",
      "Ep 1 (Step 000080): Train loss 0.605, Val loss 0.725\n",
      "Ep 1 (Step 000085): Train loss 0.509, Val loss 0.709\n",
      "Ep 1 (Step 000090): Train loss 0.562, Val loss 0.691\n",
      "Ep 1 (Step 000095): Train loss 0.500, Val loss 0.681\n",
      "Ep 1 (Step 000100): Train loss 0.503, Val loss 0.677\n",
      "Ep 1 (Step 000105): Train loss 0.564, Val loss 0.670\n",
      "Ep 1 (Step 000110): Train loss 0.555, Val loss 0.666\n",
      "Ep 1 (Step 000115): Train loss 0.508, Val loss 0.664\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive:\n",
      "Training completed in 14.18 minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5, \n",
    "    start_context=format_input(val_data[0]),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30e30351",
   "metadata": {},
   "outputs": [],
   "source": [
    "### As we can see, the model trains well based on the decreasing training loss and validation loss values.\n",
    "### Furthermore, based on the response text printed after each epoch, we can see that the model almost correctly follows the\n",
    "### instruction to convert the input sentence 'The chef cooks the meal every day' into passive voice 'The meal is prepared every day\n",
    "### by the chef'\n",
    "### To get better results, we need to finetune the model for more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae514d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    \n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # Only shows integer labels on x-axis\n",
    "    \n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()   # Creates a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)    # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df643489",
   "metadata": {},
   "outputs": [],
   "source": [
    "### As we can see in the loss plot, the models performance on both the training and validation sets improves substantially over \n",
    "### the course of training.\n",
    "### The rapid decrease in losses during the initial phase indicates that the model is quickly learning meaningful patterns and \n",
    "### representations from the data. Then, as training progresses to the second epoch, the losses continue to decrease but at a \n",
    "### slower rate, suggesting that the model is finetuning its learned representations and converging to a stable solution.\n",
    "### While the loss plot indicates that model is training effectively, the most crucial aspect is its performance in terms of\n",
    "### response quality and correctness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
