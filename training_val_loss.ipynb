{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4988461",
   "metadata": {},
   "source": [
    "### Training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "698d2ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e999567",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3033d453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d79d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c771880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a3152e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        \n",
    "        # Use a sliding window to chunk the bok into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd53f1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    \n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c566c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256,  # Context length (orig: 1024)\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers (transformers)\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-key-value bias\n",
    "}\n",
    "\n",
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4e6148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"\"\"Not enough tokens for the training loader.\n",
    "          Try to lower the GPT_CONFIG_124M['context_length'] or \n",
    "          increase the training_ratio.\"\"\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"\"\"Not enough tokens for the validation loader.\n",
    "          Try to lower the GPT_CONFIG_124M['context_length'] or \n",
    "          increase the training_ratio.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b9c38f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b954116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        \n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        # We implicitly split the matrix by adding a 'num_heads' dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        \n",
    "        # Compute scaled dot product attention with causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)    # Dot product for each head\n",
    "        \n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        \n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        \n",
    "        return context_vec\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),  # Expansion\n",
    "            GELU(), # Activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),  # Contraction\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)     # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut    # Add the original input back\n",
    "        \n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut    # Add the original input back\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "        \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c86242c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval(); # Disabling dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdf709f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcc2a995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c5f2cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583584255642\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847c66c",
   "metadata": {},
   "source": [
    "### Training loop for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bbb6cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # Add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # Remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45fac19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceed the supported context size\n",
    "        # ex: if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "            \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, voacb_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "        \n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)   # (batch, 1)\n",
    "        \n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)    # (batch, n_tokens + 1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3957d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a895e396",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The evaluate model function calculates the loss over the training and validation set while ensuring the model is in eval mode\n",
    "### with gradient tracking and dropout disabled when calculating the loss over the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0d25e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, \n",
    "            idx=encoded,\n",
    "            max_new_tokens=50,\n",
    "            context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fecb18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The generate_and_print_sample function is a convenience function that we can use to track whether the model improves during\n",
    "### the training.\n",
    "### in particular, the generate_and_print_sample function takes a text snippet (start_context)as input, converts it into token IDs\n",
    "### and feeds it into the LLM to generate a text sample using the generate_text_simple function we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d117ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    \n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()   # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()   # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step()    # Updates model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()  # Returns the total number of elements (or tokens) in the input_batch\n",
    "            global_step += 1\n",
    "            \n",
    "            # optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "        \n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dc4966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: Initialize lists to track losses and tokens seen\n",
    "### Step 2: Stat the main training loop\n",
    "### Step 3: Reset loss gradients from previous batch iteration\n",
    "### Step 4: Calculate loss gradients\n",
    "### Step 5: Update model weights using loss gradients\n",
    "### Step 6: Optional evaluation step\n",
    "### Step 7: Print a sample text after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e111fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.819, Val loss 9.925\n",
      "Ep 1 (Step 000005): Train loss 8.067, Val loss 8.339\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.623, Val loss 7.051\n",
      "Ep 2 (Step 000015): Train loss 6.047, Val loss 6.602\n",
      "Every effort moves you, and,, and,, and,,,, and,.                                   \n",
      "Ep 3 (Step 000020): Train loss 5.574, Val loss 6.488\n",
      "Ep 3 (Step 000025): Train loss 5.500, Val loss 6.400\n",
      "Every effort moves you, and, and, and, and, and, and, and. G. Gis, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,\n",
      "Ep 4 (Step 000030): Train loss 5.086, Val loss 6.328\n",
      "Ep 4 (Step 000035): Train loss 4.857, Val loss 6.333\n",
      "Every effort moves you, and a. Gisburn, and a, and a.              \"I\"--as Jack a of the of the of the of the picture\"I had been the of\n",
      "Ep 5 (Step 000040): Train loss 4.313, Val loss 6.252\n",
      "Every effort moves you, I felt, and I felt--I had the fact a little of the honour of the picture of the fact of the house of the of the      \"Oh, I had the donkey of the fact--and it's\n",
      "Ep 6 (Step 000045): Train loss 3.878, Val loss 6.167\n",
      "Ep 6 (Step 000050): Train loss 3.344, Val loss 6.141\n",
      "Every effort moves you know the                                                \n",
      "Ep 7 (Step 000055): Train loss 3.313, Val loss 6.190\n",
      "Ep 7 (Step 000060): Train loss 2.568, Val loss 6.120\n",
      "Every effort moves you know the picture, and pushed one of the deep arm-chairs.  \"I looked--and--and here are the Riv, and I was his pictures--I looked.     \"I looked up.   \n",
      "Ep 8 (Step 000065): Train loss 2.097, Val loss 6.135\n",
      "Ep 8 (Step 000070): Train loss 1.765, Val loss 6.213\n",
      "Every effort moves you know,\" was not that the picture for a smile that lifted the fact of a deprec of an exquisburn's an unusual degree to the display of his close grayish beard--as if he had the donkey. \"There were days when I\n",
      "Ep 9 (Step 000075): Train loss 1.389, Val loss 6.228\n",
      "Ep 9 (Step 000080): Train loss 1.070, Val loss 6.225\n",
      "Every effort moves you know,\" was not that my hostess was \"interesting\": on that I felt not till I felt to me!\"     \"Oh, and I looked at the donkey.        \"Oh, I\n",
      "Ep 10 (Step 000085): Train loss 0.807, Val loss 6.316\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Training completed in 0.36 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=\"Every effort moves you\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af3e11b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    \n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # Only shows integer labels on x-axis\n",
    "    \n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()   # Creates a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)    # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51dc757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Both the training and validation losses start to improve for the first epoch. However, the losses start to diverge past the\n",
    "### second epoch.\n",
    "### This divergence and the fact that the validation loss is much larger than the training loss indicates that the model is \n",
    "### overfitting to the training data.\n",
    "### We can confirm that the model memorizes the training data verbatim by searching for the generated text snippets, such as \n",
    "### \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\" in the text file.\n",
    "### This memorization is expected since we are working with a very small training dataset and training the model for multiple epochs.\n",
    "### Usually, it is common to train a model on a much much larger dataset for only one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9d5a1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acb3dd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3611324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    \n",
    "    # For-loop is the same as before: get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "    \n",
    "        # New: filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float(\"-inf\")).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        \n",
    "        # New: apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "                \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)   # (batch_size, context_len)\n",
    "                \n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "        # Otherwise same as before: get the idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)   # (batch_size, 1)\n",
    "            \n",
    "        if idx_next == eos_id:  # Stop generating early if end of sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "        \n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1) # (batch_size, num_tokens+1)\n",
    "        \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06fecdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you stand,\" she up surprise. Thwing at! She sent for reproduction in\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60896cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Overfitting problem is solved. The generated text is very different from the one we previously generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2144355",
   "metadata": {},
   "source": [
    "### Loading and Saving model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4233b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\") # .pth is a convention for pytorch files, though we could technically use any file extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7410055e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44aaebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adaptive optimizers such as AdamW store additional parameters for each model weight. AdamW uses historical data to adjust\n",
    "### learning rates for each model parameter dynamically.\n",
    "### Without it, the optimizer resets and the model may learn suboptimally or even fail to converge properly, which means that it\n",
    "### will lose the ability to generate coherent text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57bc6a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "}, \n",
    "           \"model_and_optimizer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d9b400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141921ff",
   "metadata": {},
   "source": [
    "### Loading pre-trained OpenAI weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a39cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow>=2.15.0 tqdm>=4.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a0aaa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.19.0\n",
      "tqdm version: 4.67.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(\"tqdm version:\", tqdm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b4989c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
