{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4988461",
   "metadata": {},
   "source": [
    "### Training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "698d2ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e999567",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"sherlock-holmes.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3033d453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "                        THE ADVENTURES OF SHERLOCK HOLMES\n",
      "\n",
      "                               Arthu\n"
     ]
    }
   ],
   "source": [
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82d79d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   additional information about it.\n",
      "\n",
      "     This text comes from the collection's version 3.1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1c771880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 610871\n",
      "Tokens: 179388\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a3152e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        \n",
    "        # Use a sliding window to chunk the bok into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd53f1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    \n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c566c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256,  # Context length (orig: 1024)\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers (transformers)\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-key-value bias\n",
    "}\n",
    "\n",
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a4e6148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"\"\"Not enough tokens for the training loader.\n",
    "          Try to lower the GPT_CONFIG_124M['context_length'] or \n",
    "          increase the training_ratio.\"\"\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"\"\"Not enough tokens for the validation loader.\n",
    "          Try to lower the GPT_CONFIG_124M['context_length'] or \n",
    "          increase the training_ratio.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4b9c38f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([1, 256]) torch.Size([1, 256])\n",
      "315\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b954116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        \n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        # We implicitly split the matrix by adding a 'num_heads' dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        \n",
    "        # Compute scaled dot product attention with causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)    # Dot product for each head\n",
    "        \n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        \n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        \n",
    "        return context_vec\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),  # Expansion\n",
    "            GELU(), # Activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),  # Contraction\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)     # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut    # Add the original input back\n",
    "        \n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut    # Add the original input back\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "        \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c86242c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval(); # Disabling dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cdf709f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fcc2a995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5c5f2cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 11.034156393626379\n",
      "Validation loss: 11.038600049700056\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847c66c",
   "metadata": {},
   "source": [
    "### Training loop for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9bbb6cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # Add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # Remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "45fac19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceed the supported context size\n",
    "        # ex: if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "            \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, voacb_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "        \n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)   # (batch, 1)\n",
    "        \n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)    # (batch, n_tokens + 1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d3957d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a895e396",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The evaluate model function calculates the loss over the training and validation set while ensuring the model is in eval mode\n",
    "### with gradient tracking and dropout disabled when calculating the loss over the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d0d25e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, \n",
    "            idx=encoded,\n",
    "            max_new_tokens=50,\n",
    "            context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1fecb18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The generate_and_print_sample function is a convenience function that we can use to track whether the model improves during\n",
    "### the training.\n",
    "### in particular, the generate_and_print_sample function takes a text snippet (start_context)as input, converts it into token IDs\n",
    "### and feeds it into the LLM to generate a text sample using the generate_text_simple function we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d117ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    \n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()   # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()   # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step()    # Updates model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()  # Returns the total number of elements (or tokens) in the input_batch\n",
    "            global_step += 1\n",
    "            \n",
    "            # optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "        \n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5dc4966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: Initialize lists to track losses and tokens seen\n",
    "### Step 2: Stat the main training loop\n",
    "### Step 3: Reset loss gradients from previous batch iteration\n",
    "### Step 4: Calculate loss gradients\n",
    "### Step 5: Update model weights using loss gradients\n",
    "### Step 6: Optional evaluation step\n",
    "### Step 7: Print a sample text after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0e111fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 8.325, Val loss 8.242\n",
      "Ep 1 (Step 000005): Train loss 6.687, Val loss 6.640\n",
      "Ep 1 (Step 000010): Train loss 5.874, Val loss 5.900\n",
      "Ep 1 (Step 000015): Train loss 5.488, Val loss 5.727\n",
      "Ep 1 (Step 000020): Train loss 5.685, Val loss 5.720\n",
      "Ep 1 (Step 000025): Train loss 5.513, Val loss 5.635\n",
      "Ep 1 (Step 000030): Train loss 5.504, Val loss 5.575\n",
      "Ep 1 (Step 000035): Train loss 5.434, Val loss 5.539\n",
      "Ep 1 (Step 000040): Train loss 5.316, Val loss 5.506\n",
      "Ep 1 (Step 000045): Train loss 5.416, Val loss 5.461\n",
      "Ep 1 (Step 000050): Train loss 4.992, Val loss 5.451\n",
      "Ep 1 (Step 000055): Train loss 5.273, Val loss 5.418\n",
      "Ep 1 (Step 000060): Train loss 5.109, Val loss 5.400\n",
      "Ep 1 (Step 000065): Train loss 5.054, Val loss 5.378\n",
      "Ep 1 (Step 000070): Train loss 5.140, Val loss 5.350\n",
      "Ep 1 (Step 000075): Train loss 5.174, Val loss 5.365\n",
      "Ep 1 (Step 000080): Train loss 5.071, Val loss 5.287\n",
      "Ep 1 (Step 000085): Train loss 4.816, Val loss 5.267\n",
      "Ep 1 (Step 000090): Train loss 4.773, Val loss 5.221\n",
      "Ep 1 (Step 000095): Train loss 5.054, Val loss 5.226\n",
      "Ep 1 (Step 000100): Train loss 4.730, Val loss 5.153\n",
      "Ep 1 (Step 000105): Train loss 4.880, Val loss 5.110\n",
      "Ep 1 (Step 000110): Train loss 4.886, Val loss 5.114\n",
      "Ep 1 (Step 000115): Train loss 4.828, Val loss 5.085\n",
      "Ep 1 (Step 000120): Train loss 4.927, Val loss 5.144\n",
      "Ep 1 (Step 000125): Train loss 4.989, Val loss 5.060\n",
      "Ep 1 (Step 000130): Train loss 4.813, Val loss 5.044\n",
      "Ep 1 (Step 000135): Train loss 4.647, Val loss 4.994\n",
      "Ep 1 (Step 000140): Train loss 4.617, Val loss 5.003\n",
      "Ep 1 (Step 000145): Train loss 4.743, Val loss 4.970\n",
      "Ep 1 (Step 000150): Train loss 4.812, Val loss 4.915\n",
      "Ep 1 (Step 000155): Train loss 4.803, Val loss 4.915\n",
      "Ep 1 (Step 000160): Train loss 4.705, Val loss 4.919\n",
      "Ep 1 (Step 000165): Train loss 4.537, Val loss 4.911\n",
      "Ep 1 (Step 000170): Train loss 4.357, Val loss 4.922\n",
      "Ep 1 (Step 000175): Train loss 4.447, Val loss 4.899\n",
      "Ep 1 (Step 000180): Train loss 4.751, Val loss 4.921\n",
      "Ep 1 (Step 000185): Train loss 4.731, Val loss 4.864\n",
      "Ep 1 (Step 000190): Train loss 4.364, Val loss 4.858\n",
      "Ep 1 (Step 000195): Train loss 4.524, Val loss 4.867\n",
      "Ep 1 (Step 000200): Train loss 4.392, Val loss 4.873\n",
      "Ep 1 (Step 000205): Train loss 4.575, Val loss 4.850\n",
      "Ep 1 (Step 000210): Train loss 4.629, Val loss 4.836\n",
      "Ep 1 (Step 000215): Train loss 4.479, Val loss 4.841\n",
      "Ep 1 (Step 000220): Train loss 4.445, Val loss 4.825\n",
      "Ep 1 (Step 000225): Train loss 4.548, Val loss 4.824\n",
      "Ep 1 (Step 000230): Train loss 4.498, Val loss 4.796\n",
      "Ep 1 (Step 000235): Train loss 4.570, Val loss 4.793\n",
      "Ep 1 (Step 000240): Train loss 4.516, Val loss 4.805\n",
      "Ep 1 (Step 000245): Train loss 4.134, Val loss 4.792\n",
      "Ep 1 (Step 000250): Train loss 4.492, Val loss 4.777\n",
      "Ep 1 (Step 000255): Train loss 4.178, Val loss 4.784\n",
      "Ep 1 (Step 000260): Train loss 4.334, Val loss 4.779\n",
      "Ep 1 (Step 000265): Train loss 4.398, Val loss 4.767\n",
      "Ep 1 (Step 000270): Train loss 4.569, Val loss 4.782\n",
      "Ep 1 (Step 000275): Train loss 4.701, Val loss 4.768\n",
      "Ep 1 (Step 000280): Train loss 4.426, Val loss 4.750\n",
      "Ep 1 (Step 000285): Train loss 4.402, Val loss 4.758\n",
      "Ep 1 (Step 000290): Train loss 4.562, Val loss 4.767\n",
      "Ep 1 (Step 000295): Train loss 4.453, Val loss 4.750\n",
      "Ep 1 (Step 000300): Train loss 4.431, Val loss 4.748\n",
      "Ep 1 (Step 000305): Train loss 4.286, Val loss 4.751\n",
      "Ep 1 (Step 000310): Train loss 4.359, Val loss 4.731\n",
      "Every effort moves you                                                  \n",
      "Ep 2 (Step 000315): Train loss 4.382, Val loss 4.738\n",
      "Ep 2 (Step 000320): Train loss 4.318, Val loss 4.740\n",
      "Ep 2 (Step 000325): Train loss 4.211, Val loss 4.734\n",
      "Ep 2 (Step 000330): Train loss 4.387, Val loss 4.738\n",
      "Ep 2 (Step 000335): Train loss 4.286, Val loss 4.748\n",
      "Ep 2 (Step 000340): Train loss 4.240, Val loss 4.739\n",
      "Ep 2 (Step 000345): Train loss 4.348, Val loss 4.743\n",
      "Ep 2 (Step 000350): Train loss 4.190, Val loss 4.744\n",
      "Ep 2 (Step 000355): Train loss 4.477, Val loss 4.745\n",
      "Ep 2 (Step 000360): Train loss 4.326, Val loss 4.752\n",
      "Ep 2 (Step 000365): Train loss 4.373, Val loss 4.732\n",
      "Ep 2 (Step 000370): Train loss 4.272, Val loss 4.733\n",
      "Ep 2 (Step 000375): Train loss 4.082, Val loss 4.743\n",
      "Ep 2 (Step 000380): Train loss 4.237, Val loss 4.736\n",
      "Ep 2 (Step 000385): Train loss 4.166, Val loss 4.730\n",
      "Ep 2 (Step 000390): Train loss 4.371, Val loss 4.735\n",
      "Ep 2 (Step 000395): Train loss 4.351, Val loss 4.733\n",
      "Ep 2 (Step 000400): Train loss 4.207, Val loss 4.756\n",
      "Ep 2 (Step 000405): Train loss 4.287, Val loss 4.742\n",
      "Ep 2 (Step 000410): Train loss 4.141, Val loss 4.733\n",
      "Ep 2 (Step 000415): Train loss 4.111, Val loss 4.736\n",
      "Ep 2 (Step 000420): Train loss 4.101, Val loss 4.743\n",
      "Ep 2 (Step 000425): Train loss 4.362, Val loss 4.731\n",
      "Ep 2 (Step 000430): Train loss 4.113, Val loss 4.722\n",
      "Ep 2 (Step 000435): Train loss 4.219, Val loss 4.713\n",
      "Ep 2 (Step 000440): Train loss 4.396, Val loss 4.702\n",
      "Ep 2 (Step 000445): Train loss 4.187, Val loss 4.717\n",
      "Ep 2 (Step 000450): Train loss 4.177, Val loss 4.714\n",
      "Ep 2 (Step 000455): Train loss 4.097, Val loss 4.698\n",
      "Ep 2 (Step 000460): Train loss 4.152, Val loss 4.705\n",
      "Ep 2 (Step 000465): Train loss 4.059, Val loss 4.727\n",
      "Ep 2 (Step 000470): Train loss 4.286, Val loss 4.702\n",
      "Ep 2 (Step 000475): Train loss 4.351, Val loss 4.705\n",
      "Ep 2 (Step 000480): Train loss 3.977, Val loss 4.680\n",
      "Ep 2 (Step 000485): Train loss 4.204, Val loss 4.686\n",
      "Ep 2 (Step 000490): Train loss 4.145, Val loss 4.682\n",
      "Ep 2 (Step 000495): Train loss 4.390, Val loss 4.687\n",
      "Ep 2 (Step 000500): Train loss 4.015, Val loss 4.673\n",
      "Ep 2 (Step 000505): Train loss 4.189, Val loss 4.679\n",
      "Ep 2 (Step 000510): Train loss 4.123, Val loss 4.673\n",
      "Ep 2 (Step 000515): Train loss 4.259, Val loss 4.667\n",
      "Ep 2 (Step 000520): Train loss 4.125, Val loss 4.677\n",
      "Ep 2 (Step 000525): Train loss 4.231, Val loss 4.665\n",
      "Ep 2 (Step 000530): Train loss 4.076, Val loss 4.651\n",
      "Ep 2 (Step 000535): Train loss 4.028, Val loss 4.667\n",
      "Ep 2 (Step 000540): Train loss 4.147, Val loss 4.658\n",
      "Ep 2 (Step 000545): Train loss 4.255, Val loss 4.633\n",
      "Ep 2 (Step 000550): Train loss 4.185, Val loss 4.622\n",
      "Ep 2 (Step 000555): Train loss 3.885, Val loss 4.626\n",
      "Ep 2 (Step 000560): Train loss 4.184, Val loss 4.638\n",
      "Ep 2 (Step 000565): Train loss 4.130, Val loss 4.627\n",
      "Ep 2 (Step 000570): Train loss 4.192, Val loss 4.606\n",
      "Ep 2 (Step 000575): Train loss 3.995, Val loss 4.613\n",
      "Ep 2 (Step 000580): Train loss 4.181, Val loss 4.639\n",
      "Ep 2 (Step 000585): Train loss 3.902, Val loss 4.634\n",
      "Ep 2 (Step 000590): Train loss 4.070, Val loss 4.618\n",
      "Ep 2 (Step 000595): Train loss 3.961, Val loss 4.647\n",
      "Ep 2 (Step 000600): Train loss 4.134, Val loss 4.630\n",
      "Ep 2 (Step 000605): Train loss 4.168, Val loss 4.620\n",
      "Ep 2 (Step 000610): Train loss 4.034, Val loss 4.607\n",
      "Ep 2 (Step 000615): Train loss 4.098, Val loss 4.627\n",
      "Ep 2 (Step 000620): Train loss 4.013, Val loss 4.612\n",
      "Ep 2 (Step 000625): Train loss 4.048, Val loss 4.623\n",
      "Every effort moves you have been                                                \n",
      "Ep 3 (Step 000630): Train loss 4.090, Val loss 4.610\n",
      "Ep 3 (Step 000635): Train loss 4.028, Val loss 4.604\n",
      "Ep 3 (Step 000640): Train loss 4.070, Val loss 4.621\n",
      "Ep 3 (Step 000645): Train loss 4.109, Val loss 4.619\n",
      "Ep 3 (Step 000650): Train loss 4.002, Val loss 4.626\n",
      "Ep 3 (Step 000655): Train loss 4.010, Val loss 4.643\n",
      "Ep 3 (Step 000660): Train loss 4.072, Val loss 4.625\n",
      "Ep 3 (Step 000665): Train loss 4.047, Val loss 4.658\n",
      "Ep 3 (Step 000670): Train loss 4.335, Val loss 4.656\n",
      "Ep 3 (Step 000675): Train loss 4.039, Val loss 4.653\n",
      "Ep 3 (Step 000680): Train loss 3.884, Val loss 4.626\n",
      "Ep 3 (Step 000685): Train loss 3.885, Val loss 4.641\n",
      "Ep 3 (Step 000690): Train loss 4.232, Val loss 4.639\n",
      "Ep 3 (Step 000695): Train loss 3.954, Val loss 4.633\n",
      "Ep 3 (Step 000700): Train loss 4.035, Val loss 4.669\n",
      "Ep 3 (Step 000705): Train loss 3.836, Val loss 4.649\n",
      "Ep 3 (Step 000710): Train loss 3.967, Val loss 4.643\n",
      "Ep 3 (Step 000715): Train loss 4.070, Val loss 4.666\n",
      "Ep 3 (Step 000720): Train loss 4.098, Val loss 4.645\n",
      "Ep 3 (Step 000725): Train loss 3.967, Val loss 4.641\n",
      "Ep 3 (Step 000730): Train loss 3.862, Val loss 4.617\n",
      "Ep 3 (Step 000735): Train loss 3.995, Val loss 4.619\n",
      "Ep 3 (Step 000740): Train loss 4.028, Val loss 4.610\n",
      "Ep 3 (Step 000745): Train loss 3.830, Val loss 4.592\n",
      "Ep 3 (Step 000750): Train loss 4.059, Val loss 4.587\n",
      "Ep 3 (Step 000755): Train loss 4.040, Val loss 4.612\n",
      "Ep 3 (Step 000760): Train loss 4.074, Val loss 4.595\n",
      "Ep 3 (Step 000765): Train loss 4.152, Val loss 4.595\n",
      "Ep 3 (Step 000770): Train loss 3.810, Val loss 4.595\n",
      "Ep 3 (Step 000775): Train loss 4.068, Val loss 4.600\n",
      "Ep 3 (Step 000780): Train loss 3.900, Val loss 4.617\n",
      "Ep 3 (Step 000785): Train loss 3.978, Val loss 4.609\n",
      "Ep 3 (Step 000790): Train loss 3.942, Val loss 4.621\n",
      "Ep 3 (Step 000795): Train loss 3.855, Val loss 4.621\n",
      "Ep 3 (Step 000800): Train loss 4.023, Val loss 4.602\n",
      "Ep 3 (Step 000805): Train loss 3.911, Val loss 4.608\n",
      "Ep 3 (Step 000810): Train loss 3.880, Val loss 4.603\n",
      "Ep 3 (Step 000815): Train loss 4.015, Val loss 4.611\n",
      "Ep 3 (Step 000820): Train loss 3.900, Val loss 4.617\n",
      "Ep 3 (Step 000825): Train loss 3.869, Val loss 4.593\n",
      "Ep 3 (Step 000830): Train loss 3.905, Val loss 4.593\n",
      "Ep 3 (Step 000835): Train loss 3.937, Val loss 4.577\n",
      "Ep 3 (Step 000840): Train loss 3.935, Val loss 4.566\n",
      "Ep 3 (Step 000845): Train loss 3.574, Val loss 4.580\n",
      "Ep 3 (Step 000850): Train loss 3.986, Val loss 4.583\n",
      "Ep 3 (Step 000855): Train loss 3.743, Val loss 4.587\n",
      "Ep 3 (Step 000860): Train loss 3.899, Val loss 4.564\n",
      "Ep 3 (Step 000865): Train loss 3.901, Val loss 4.538\n",
      "Ep 3 (Step 000870): Train loss 3.823, Val loss 4.537\n",
      "Ep 3 (Step 000875): Train loss 3.816, Val loss 4.564\n",
      "Ep 3 (Step 000880): Train loss 3.852, Val loss 4.557\n",
      "Ep 3 (Step 000885): Train loss 3.771, Val loss 4.540\n",
      "Ep 3 (Step 000890): Train loss 3.907, Val loss 4.544\n",
      "Ep 3 (Step 000895): Train loss 3.858, Val loss 4.527\n",
      "Ep 3 (Step 000900): Train loss 3.647, Val loss 4.524\n",
      "Ep 3 (Step 000905): Train loss 3.952, Val loss 4.555\n",
      "Ep 3 (Step 000910): Train loss 3.859, Val loss 4.528\n",
      "Ep 3 (Step 000915): Train loss 3.709, Val loss 4.517\n",
      "Ep 3 (Step 000920): Train loss 3.904, Val loss 4.503\n",
      "Ep 3 (Step 000925): Train loss 3.842, Val loss 4.502\n",
      "Ep 3 (Step 000930): Train loss 3.837, Val loss 4.539\n",
      "Ep 3 (Step 000935): Train loss 3.958, Val loss 4.514\n",
      "Ep 3 (Step 000940): Train loss 3.815, Val loss 4.522\n",
      "Every effort moves you, and the                                               \n",
      "Ep 4 (Step 000945): Train loss 3.791, Val loss 4.520\n",
      "Ep 4 (Step 000950): Train loss 3.807, Val loss 4.559\n",
      "Ep 4 (Step 000955): Train loss 3.690, Val loss 4.562\n",
      "Ep 4 (Step 000960): Train loss 3.816, Val loss 4.503\n",
      "Ep 4 (Step 000965): Train loss 3.643, Val loss 4.556\n",
      "Ep 4 (Step 000970): Train loss 3.523, Val loss 4.571\n",
      "Ep 4 (Step 000975): Train loss 3.680, Val loss 4.549\n",
      "Ep 4 (Step 000980): Train loss 3.873, Val loss 4.511\n",
      "Ep 4 (Step 000985): Train loss 3.863, Val loss 4.532\n",
      "Ep 4 (Step 000990): Train loss 3.678, Val loss 4.562\n",
      "Ep 4 (Step 000995): Train loss 3.776, Val loss 4.523\n",
      "Ep 4 (Step 001000): Train loss 3.765, Val loss 4.503\n",
      "Ep 4 (Step 001005): Train loss 3.687, Val loss 4.503\n",
      "Ep 4 (Step 001010): Train loss 3.700, Val loss 4.526\n",
      "Ep 4 (Step 001015): Train loss 3.438, Val loss 4.502\n",
      "Ep 4 (Step 001020): Train loss 3.778, Val loss 4.491\n",
      "Ep 4 (Step 001025): Train loss 3.635, Val loss 4.506\n",
      "Ep 4 (Step 001030): Train loss 3.572, Val loss 4.512\n",
      "Ep 4 (Step 001035): Train loss 3.770, Val loss 4.514\n",
      "Ep 4 (Step 001040): Train loss 3.800, Val loss 4.524\n",
      "Ep 4 (Step 001045): Train loss 3.639, Val loss 4.581\n",
      "Ep 4 (Step 001050): Train loss 3.692, Val loss 4.539\n",
      "Ep 4 (Step 001055): Train loss 3.520, Val loss 4.512\n",
      "Ep 4 (Step 001060): Train loss 3.859, Val loss 4.525\n",
      "Ep 4 (Step 001065): Train loss 3.542, Val loss 4.507\n",
      "Ep 4 (Step 001070): Train loss 3.765, Val loss 4.505\n",
      "Ep 4 (Step 001075): Train loss 3.728, Val loss 4.495\n",
      "Ep 4 (Step 001080): Train loss 3.711, Val loss 4.514\n",
      "Ep 4 (Step 001085): Train loss 3.648, Val loss 4.492\n",
      "Ep 4 (Step 001090): Train loss 3.504, Val loss 4.470\n",
      "Ep 4 (Step 001095): Train loss 3.556, Val loss 4.528\n",
      "Ep 4 (Step 001100): Train loss 3.739, Val loss 4.534\n",
      "Ep 4 (Step 001105): Train loss 3.588, Val loss 4.493\n",
      "Ep 4 (Step 001110): Train loss 3.545, Val loss 4.461\n",
      "Ep 4 (Step 001115): Train loss 3.555, Val loss 4.467\n",
      "Ep 4 (Step 001120): Train loss 3.788, Val loss 4.491\n",
      "Ep 4 (Step 001125): Train loss 3.597, Val loss 4.472\n",
      "Ep 4 (Step 001130): Train loss 3.465, Val loss 4.468\n",
      "Ep 4 (Step 001135): Train loss 3.572, Val loss 4.473\n",
      "Ep 4 (Step 001140): Train loss 3.662, Val loss 4.465\n",
      "Ep 4 (Step 001145): Train loss 3.595, Val loss 4.448\n",
      "Ep 4 (Step 001150): Train loss 3.505, Val loss 4.457\n",
      "Ep 4 (Step 001155): Train loss 3.545, Val loss 4.457\n",
      "Ep 4 (Step 001160): Train loss 3.774, Val loss 4.475\n",
      "Ep 4 (Step 001165): Train loss 3.707, Val loss 4.447\n",
      "Ep 4 (Step 001170): Train loss 3.366, Val loss 4.449\n",
      "Ep 4 (Step 001175): Train loss 3.666, Val loss 4.441\n",
      "Ep 4 (Step 001180): Train loss 3.580, Val loss 4.469\n",
      "Ep 4 (Step 001185): Train loss 3.447, Val loss 4.471\n",
      "Ep 4 (Step 001190): Train loss 3.627, Val loss 4.483\n",
      "Ep 4 (Step 001195): Train loss 3.492, Val loss 4.485\n",
      "Ep 4 (Step 001200): Train loss 3.451, Val loss 4.470\n",
      "Ep 4 (Step 001205): Train loss 3.664, Val loss 4.455\n",
      "Ep 4 (Step 001210): Train loss 3.577, Val loss 4.466\n",
      "Ep 4 (Step 001215): Train loss 3.519, Val loss 4.447\n",
      "Ep 4 (Step 001220): Train loss 3.536, Val loss 4.433\n",
      "Ep 4 (Step 001225): Train loss 3.599, Val loss 4.436\n",
      "Ep 4 (Step 001230): Train loss 3.350, Val loss 4.453\n",
      "Ep 4 (Step 001235): Train loss 3.574, Val loss 4.463\n",
      "Ep 4 (Step 001240): Train loss 3.527, Val loss 4.494\n",
      "Ep 4 (Step 001245): Train loss 3.219, Val loss 4.439\n",
      "Ep 4 (Step 001250): Train loss 3.634, Val loss 4.435\n",
      "Ep 4 (Step 001255): Train loss 3.645, Val loss 4.452\n",
      "Every effort moves you, and      the      the      the      the      the      the      the      the\n",
      "Ep 5 (Step 001260): Train loss 3.414, Val loss 4.438\n",
      "Ep 5 (Step 001265): Train loss 3.398, Val loss 4.446\n",
      "Ep 5 (Step 001270): Train loss 3.588, Val loss 4.492\n",
      "Ep 5 (Step 001275): Train loss 3.363, Val loss 4.516\n",
      "Ep 5 (Step 001280): Train loss 3.481, Val loss 4.536\n",
      "Ep 5 (Step 001285): Train loss 3.508, Val loss 4.517\n",
      "Ep 5 (Step 001290): Train loss 3.373, Val loss 4.500\n",
      "Ep 5 (Step 001295): Train loss 3.373, Val loss 4.494\n",
      "Ep 5 (Step 001300): Train loss 3.446, Val loss 4.515\n",
      "Ep 5 (Step 001305): Train loss 3.467, Val loss 4.535\n",
      "Ep 5 (Step 001310): Train loss 3.360, Val loss 4.560\n",
      "Ep 5 (Step 001315): Train loss 3.432, Val loss 4.550\n",
      "Ep 5 (Step 001320): Train loss 3.443, Val loss 4.557\n",
      "Ep 5 (Step 001325): Train loss 3.409, Val loss 4.543\n",
      "Ep 5 (Step 001330): Train loss 3.477, Val loss 4.525\n",
      "Ep 5 (Step 001335): Train loss 3.403, Val loss 4.488\n",
      "Ep 5 (Step 001340): Train loss 3.231, Val loss 4.501\n",
      "Ep 5 (Step 001345): Train loss 3.403, Val loss 4.540\n",
      "Ep 5 (Step 001350): Train loss 3.297, Val loss 4.543\n",
      "Ep 5 (Step 001355): Train loss 3.585, Val loss 4.525\n",
      "Ep 5 (Step 001360): Train loss 3.440, Val loss 4.523\n",
      "Ep 5 (Step 001365): Train loss 3.376, Val loss 4.515\n",
      "Ep 5 (Step 001370): Train loss 3.363, Val loss 4.529\n",
      "Ep 5 (Step 001375): Train loss 3.465, Val loss 4.527\n",
      "Ep 5 (Step 001380): Train loss 3.466, Val loss 4.527\n",
      "Ep 5 (Step 001385): Train loss 3.448, Val loss 4.512\n",
      "Ep 5 (Step 001390): Train loss 3.338, Val loss 4.510\n",
      "Ep 5 (Step 001395): Train loss 3.348, Val loss 4.514\n",
      "Ep 5 (Step 001400): Train loss 2.943, Val loss 4.503\n",
      "Ep 5 (Step 001405): Train loss 3.235, Val loss 4.496\n",
      "Ep 5 (Step 001410): Train loss 3.418, Val loss 4.499\n",
      "Ep 5 (Step 001415): Train loss 3.442, Val loss 4.509\n",
      "Ep 5 (Step 001420): Train loss 3.187, Val loss 4.495\n",
      "Ep 5 (Step 001425): Train loss 3.303, Val loss 4.495\n",
      "Ep 5 (Step 001430): Train loss 3.473, Val loss 4.500\n",
      "Ep 5 (Step 001435): Train loss 3.238, Val loss 4.479\n",
      "Ep 5 (Step 001440): Train loss 3.209, Val loss 4.462\n",
      "Ep 5 (Step 001445): Train loss 3.393, Val loss 4.505\n",
      "Ep 5 (Step 001450): Train loss 3.548, Val loss 4.512\n",
      "Ep 5 (Step 001455): Train loss 3.379, Val loss 4.528\n",
      "Ep 5 (Step 001460): Train loss 3.511, Val loss 4.507\n",
      "Ep 5 (Step 001465): Train loss 3.157, Val loss 4.517\n",
      "Ep 5 (Step 001470): Train loss 3.056, Val loss 4.510\n",
      "Ep 5 (Step 001475): Train loss 3.345, Val loss 4.524\n",
      "Ep 5 (Step 001480): Train loss 3.126, Val loss 4.521\n",
      "Ep 5 (Step 001485): Train loss 3.361, Val loss 4.557\n",
      "Ep 5 (Step 001490): Train loss 3.351, Val loss 4.512\n",
      "Ep 5 (Step 001495): Train loss 3.324, Val loss 4.509\n",
      "Ep 5 (Step 001500): Train loss 3.458, Val loss 4.495\n",
      "Ep 5 (Step 001505): Train loss 3.258, Val loss 4.480\n",
      "Ep 5 (Step 001510): Train loss 3.286, Val loss 4.471\n",
      "Ep 5 (Step 001515): Train loss 2.987, Val loss 4.487\n",
      "Ep 5 (Step 001520): Train loss 3.338, Val loss 4.482\n",
      "Ep 5 (Step 001525): Train loss 3.224, Val loss 4.474\n",
      "Ep 5 (Step 001530): Train loss 3.209, Val loss 4.455\n",
      "Ep 5 (Step 001535): Train loss 3.370, Val loss 4.448\n",
      "Ep 5 (Step 001540): Train loss 3.330, Val loss 4.445\n",
      "Ep 5 (Step 001545): Train loss 3.235, Val loss 4.481\n",
      "Ep 5 (Step 001550): Train loss 3.190, Val loss 4.438\n",
      "Ep 5 (Step 001555): Train loss 3.189, Val loss 4.442\n",
      "Ep 5 (Step 001560): Train loss 3.316, Val loss 4.483\n",
      "Ep 5 (Step 001565): Train loss 3.289, Val loss 4.448\n",
      "Ep 5 (Step 001570): Train loss 3.271, Val loss 4.446\n",
      "Every effort moves you have been      the      the      the      the      the      the      the      the\n",
      "Ep 6 (Step 001575): Train loss 3.293, Val loss 4.442\n",
      "Ep 6 (Step 001580): Train loss 3.183, Val loss 4.483\n",
      "Ep 6 (Step 001585): Train loss 3.181, Val loss 4.538\n",
      "Ep 6 (Step 001590): Train loss 3.060, Val loss 4.565\n",
      "Ep 6 (Step 001595): Train loss 3.133, Val loss 4.549\n",
      "Ep 6 (Step 001600): Train loss 3.260, Val loss 4.564\n",
      "Ep 6 (Step 001605): Train loss 3.321, Val loss 4.522\n",
      "Ep 6 (Step 001610): Train loss 3.102, Val loss 4.534\n",
      "Ep 6 (Step 001615): Train loss 3.092, Val loss 4.525\n",
      "Ep 6 (Step 001620): Train loss 3.068, Val loss 4.555\n",
      "Ep 6 (Step 001625): Train loss 3.234, Val loss 4.570\n",
      "Ep 6 (Step 001630): Train loss 3.313, Val loss 4.572\n",
      "Ep 6 (Step 001635): Train loss 3.119, Val loss 4.567\n",
      "Ep 6 (Step 001640): Train loss 3.219, Val loss 4.617\n",
      "Ep 6 (Step 001645): Train loss 3.116, Val loss 4.568\n",
      "Ep 6 (Step 001650): Train loss 3.106, Val loss 4.570\n",
      "Ep 6 (Step 001655): Train loss 3.293, Val loss 4.531\n",
      "Ep 6 (Step 001660): Train loss 3.166, Val loss 4.564\n",
      "Ep 6 (Step 001665): Train loss 3.172, Val loss 4.590\n",
      "Ep 6 (Step 001670): Train loss 3.288, Val loss 4.569\n",
      "Ep 6 (Step 001675): Train loss 3.172, Val loss 4.599\n",
      "Ep 6 (Step 001680): Train loss 3.147, Val loss 4.596\n",
      "Ep 6 (Step 001685): Train loss 3.173, Val loss 4.595\n",
      "Ep 6 (Step 001690): Train loss 3.234, Val loss 4.586\n",
      "Ep 6 (Step 001695): Train loss 3.293, Val loss 4.604\n",
      "Ep 6 (Step 001700): Train loss 3.276, Val loss 4.570\n",
      "Ep 6 (Step 001705): Train loss 3.160, Val loss 4.554\n",
      "Ep 6 (Step 001710): Train loss 3.058, Val loss 4.554\n",
      "Ep 6 (Step 001715): Train loss 3.016, Val loss 4.539\n",
      "Ep 6 (Step 001720): Train loss 3.206, Val loss 4.517\n",
      "Ep 6 (Step 001725): Train loss 3.131, Val loss 4.541\n",
      "Ep 6 (Step 001730): Train loss 3.084, Val loss 4.549\n",
      "Ep 6 (Step 001735): Train loss 3.382, Val loss 4.538\n",
      "Ep 6 (Step 001740): Train loss 3.047, Val loss 4.555\n",
      "Ep 6 (Step 001745): Train loss 3.077, Val loss 4.556\n",
      "Ep 6 (Step 001750): Train loss 2.969, Val loss 4.546\n",
      "Ep 6 (Step 001755): Train loss 3.255, Val loss 4.559\n",
      "Ep 6 (Step 001760): Train loss 3.278, Val loss 4.560\n",
      "Ep 6 (Step 001765): Train loss 3.238, Val loss 4.561\n",
      "Ep 6 (Step 001770): Train loss 2.983, Val loss 4.548\n",
      "Ep 6 (Step 001775): Train loss 3.180, Val loss 4.553\n",
      "Ep 6 (Step 001780): Train loss 3.211, Val loss 4.556\n",
      "Ep 6 (Step 001785): Train loss 3.022, Val loss 4.541\n",
      "Ep 6 (Step 001790): Train loss 2.995, Val loss 4.534\n",
      "Ep 6 (Step 001795): Train loss 3.208, Val loss 4.535\n",
      "Ep 6 (Step 001800): Train loss 3.187, Val loss 4.542\n",
      "Ep 6 (Step 001805): Train loss 3.101, Val loss 4.543\n",
      "Ep 6 (Step 001810): Train loss 3.080, Val loss 4.535\n",
      "Ep 6 (Step 001815): Train loss 3.103, Val loss 4.545\n",
      "Ep 6 (Step 001820): Train loss 3.068, Val loss 4.527\n",
      "Ep 6 (Step 001825): Train loss 2.973, Val loss 4.536\n",
      "Ep 6 (Step 001830): Train loss 3.213, Val loss 4.558\n",
      "Ep 6 (Step 001835): Train loss 3.167, Val loss 4.550\n",
      "Ep 6 (Step 001840): Train loss 2.984, Val loss 4.535\n",
      "Ep 6 (Step 001845): Train loss 3.117, Val loss 4.538\n",
      "Ep 6 (Step 001850): Train loss 3.024, Val loss 4.544\n",
      "Ep 6 (Step 001855): Train loss 3.160, Val loss 4.563\n",
      "Ep 6 (Step 001860): Train loss 3.177, Val loss 4.555\n",
      "Ep 6 (Step 001865): Train loss 3.061, Val loss 4.533\n",
      "Ep 6 (Step 001870): Train loss 3.149, Val loss 4.555\n",
      "Ep 6 (Step 001875): Train loss 2.951, Val loss 4.562\n",
      "Ep 6 (Step 001880): Train loss 3.006, Val loss 4.551\n",
      "Ep 6 (Step 001885): Train loss 2.949, Val loss 4.557\n",
      "Every effort moves you will be      the authorities to the      of the      of the      the door, and the      of the      of the \n",
      "Ep 7 (Step 001890): Train loss 3.054, Val loss 4.543\n",
      "Ep 7 (Step 001895): Train loss 2.999, Val loss 4.573\n",
      "Ep 7 (Step 001900): Train loss 2.922, Val loss 4.628\n",
      "Ep 7 (Step 001905): Train loss 3.091, Val loss 4.628\n",
      "Ep 7 (Step 001910): Train loss 2.885, Val loss 4.639\n",
      "Ep 7 (Step 001915): Train loss 2.974, Val loss 4.648\n",
      "Ep 7 (Step 001920): Train loss 3.015, Val loss 4.643\n",
      "Ep 7 (Step 001925): Train loss 2.940, Val loss 4.637\n",
      "Ep 7 (Step 001930): Train loss 2.828, Val loss 4.641\n",
      "Ep 7 (Step 001935): Train loss 2.874, Val loss 4.671\n",
      "Ep 7 (Step 001940): Train loss 3.002, Val loss 4.677\n",
      "Ep 7 (Step 001945): Train loss 2.984, Val loss 4.684\n",
      "Ep 7 (Step 001950): Train loss 2.921, Val loss 4.683\n",
      "Ep 7 (Step 001955): Train loss 2.999, Val loss 4.668\n",
      "Ep 7 (Step 001960): Train loss 2.825, Val loss 4.655\n",
      "Ep 7 (Step 001965): Train loss 3.192, Val loss 4.656\n",
      "Ep 7 (Step 001970): Train loss 2.836, Val loss 4.643\n",
      "Ep 7 (Step 001975): Train loss 2.936, Val loss 4.670\n",
      "Ep 7 (Step 001980): Train loss 2.907, Val loss 4.696\n",
      "Ep 7 (Step 001985): Train loss 2.955, Val loss 4.668\n",
      "Ep 7 (Step 001990): Train loss 2.952, Val loss 4.665\n",
      "Ep 7 (Step 001995): Train loss 3.071, Val loss 4.659\n",
      "Ep 7 (Step 002000): Train loss 2.994, Val loss 4.691\n",
      "Ep 7 (Step 002005): Train loss 3.137, Val loss 4.674\n",
      "Ep 7 (Step 002010): Train loss 3.000, Val loss 4.657\n",
      "Ep 7 (Step 002015): Train loss 3.036, Val loss 4.703\n",
      "Ep 7 (Step 002020): Train loss 2.960, Val loss 4.678\n",
      "Ep 7 (Step 002025): Train loss 2.985, Val loss 4.684\n",
      "Ep 7 (Step 002030): Train loss 3.010, Val loss 4.691\n",
      "Ep 7 (Step 002035): Train loss 3.079, Val loss 4.640\n",
      "Ep 7 (Step 002040): Train loss 2.908, Val loss 4.636\n",
      "Ep 7 (Step 002045): Train loss 3.151, Val loss 4.640\n",
      "Ep 7 (Step 002050): Train loss 2.714, Val loss 4.662\n",
      "Ep 7 (Step 002055): Train loss 2.966, Val loss 4.651\n",
      "Ep 7 (Step 002060): Train loss 2.904, Val loss 4.658\n",
      "Ep 7 (Step 002065): Train loss 3.085, Val loss 4.622\n",
      "Ep 7 (Step 002070): Train loss 3.036, Val loss 4.618\n",
      "Ep 7 (Step 002075): Train loss 2.931, Val loss 4.631\n",
      "Ep 7 (Step 002080): Train loss 2.905, Val loss 4.663\n",
      "Ep 7 (Step 002085): Train loss 3.107, Val loss 4.666\n",
      "Ep 7 (Step 002090): Train loss 3.238, Val loss 4.677\n",
      "Ep 7 (Step 002095): Train loss 3.078, Val loss 4.674\n",
      "Ep 7 (Step 002100): Train loss 2.871, Val loss 4.669\n",
      "Ep 7 (Step 002105): Train loss 2.922, Val loss 4.670\n",
      "Ep 7 (Step 002110): Train loss 3.004, Val loss 4.664\n",
      "Ep 7 (Step 002115): Train loss 2.834, Val loss 4.687\n",
      "Ep 7 (Step 002120): Train loss 3.084, Val loss 4.674\n",
      "Ep 7 (Step 002125): Train loss 2.977, Val loss 4.659\n",
      "Ep 7 (Step 002130): Train loss 2.813, Val loss 4.654\n",
      "Ep 7 (Step 002135): Train loss 2.984, Val loss 4.636\n",
      "Ep 7 (Step 002140): Train loss 2.836, Val loss 4.621\n",
      "Ep 7 (Step 002145): Train loss 2.857, Val loss 4.634\n",
      "Ep 7 (Step 002150): Train loss 2.878, Val loss 4.635\n",
      "Ep 7 (Step 002155): Train loss 2.824, Val loss 4.647\n",
      "Ep 7 (Step 002160): Train loss 2.971, Val loss 4.648\n",
      "Ep 7 (Step 002165): Train loss 2.955, Val loss 4.657\n",
      "Ep 7 (Step 002170): Train loss 2.870, Val loss 4.625\n",
      "Ep 7 (Step 002175): Train loss 2.887, Val loss 4.622\n",
      "Ep 7 (Step 002180): Train loss 2.860, Val loss 4.642\n",
      "Ep 7 (Step 002185): Train loss 2.907, Val loss 4.627\n",
      "Ep 7 (Step 002190): Train loss 2.996, Val loss 4.631\n",
      "Ep 7 (Step 002195): Train loss 2.875, Val loss 4.647\n",
      "Ep 7 (Step 002200): Train loss 2.976, Val loss 4.645\n",
      "Every effort moves you      the window, and I shall be      the      the table, and I was      the   of the   the   the table, and the \n",
      "Ep 8 (Step 002205): Train loss 2.891, Val loss 4.620\n",
      "Ep 8 (Step 002210): Train loss 2.902, Val loss 4.636\n",
      "Ep 8 (Step 002215): Train loss 2.833, Val loss 4.678\n",
      "Ep 8 (Step 002220): Train loss 2.752, Val loss 4.699\n",
      "Ep 8 (Step 002225): Train loss 2.863, Val loss 4.714\n",
      "Ep 8 (Step 002230): Train loss 2.854, Val loss 4.717\n",
      "Ep 8 (Step 002235): Train loss 2.820, Val loss 4.729\n",
      "Ep 8 (Step 002240): Train loss 2.937, Val loss 4.742\n",
      "Ep 8 (Step 002245): Train loss 2.947, Val loss 4.758\n",
      "Ep 8 (Step 002250): Train loss 2.766, Val loss 4.751\n",
      "Ep 8 (Step 002255): Train loss 2.922, Val loss 4.790\n",
      "Ep 8 (Step 002260): Train loss 3.045, Val loss 4.757\n",
      "Ep 8 (Step 002265): Train loss 2.964, Val loss 4.735\n",
      "Ep 8 (Step 002270): Train loss 2.941, Val loss 4.759\n",
      "Ep 8 (Step 002275): Train loss 3.007, Val loss 4.757\n",
      "Ep 8 (Step 002280): Train loss 3.039, Val loss 4.730\n",
      "Ep 8 (Step 002285): Train loss 2.904, Val loss 4.732\n",
      "Ep 8 (Step 002290): Train loss 2.780, Val loss 4.734\n",
      "Ep 8 (Step 002295): Train loss 2.783, Val loss 4.735\n",
      "Ep 8 (Step 002300): Train loss 2.980, Val loss 4.749\n",
      "Ep 8 (Step 002305): Train loss 2.778, Val loss 4.750\n",
      "Ep 8 (Step 002310): Train loss 2.865, Val loss 4.748\n",
      "Ep 8 (Step 002315): Train loss 2.889, Val loss 4.730\n",
      "Ep 8 (Step 002320): Train loss 2.852, Val loss 4.749\n",
      "Ep 8 (Step 002325): Train loss 2.795, Val loss 4.763\n",
      "Ep 8 (Step 002330): Train loss 2.974, Val loss 4.728\n",
      "Ep 8 (Step 002335): Train loss 2.779, Val loss 4.746\n",
      "Ep 8 (Step 002340): Train loss 3.212, Val loss 4.755\n",
      "Ep 8 (Step 002345): Train loss 3.004, Val loss 4.752\n",
      "Ep 8 (Step 002350): Train loss 3.171, Val loss 4.723\n",
      "Ep 8 (Step 002355): Train loss 2.863, Val loss 4.723\n",
      "Ep 8 (Step 002360): Train loss 2.915, Val loss 4.718\n",
      "Ep 8 (Step 002365): Train loss 2.868, Val loss 4.706\n",
      "Ep 8 (Step 002370): Train loss 2.847, Val loss 4.692\n",
      "Ep 8 (Step 002375): Train loss 2.931, Val loss 4.709\n",
      "Ep 8 (Step 002380): Train loss 2.876, Val loss 4.714\n",
      "Ep 8 (Step 002385): Train loss 3.135, Val loss 4.737\n",
      "Ep 8 (Step 002390): Train loss 2.853, Val loss 4.717\n",
      "Ep 8 (Step 002395): Train loss 2.895, Val loss 4.757\n",
      "Ep 8 (Step 002400): Train loss 3.057, Val loss 4.740\n",
      "Ep 8 (Step 002405): Train loss 2.980, Val loss 4.740\n",
      "Ep 8 (Step 002410): Train loss 2.840, Val loss 4.761\n",
      "Ep 8 (Step 002415): Train loss 2.673, Val loss 4.762\n",
      "Ep 8 (Step 002420): Train loss 2.857, Val loss 4.742\n",
      "Ep 8 (Step 002425): Train loss 2.667, Val loss 4.718\n",
      "Ep 8 (Step 002430): Train loss 2.761, Val loss 4.706\n",
      "Ep 8 (Step 002435): Train loss 2.715, Val loss 4.723\n",
      "Ep 8 (Step 002440): Train loss 2.951, Val loss 4.737\n",
      "Ep 8 (Step 002445): Train loss 3.046, Val loss 4.753\n",
      "Ep 8 (Step 002450): Train loss 2.737, Val loss 4.754\n",
      "Ep 8 (Step 002455): Train loss 2.917, Val loss 4.755\n",
      "Ep 8 (Step 002460): Train loss 2.721, Val loss 4.749\n",
      "Ep 8 (Step 002465): Train loss 2.787, Val loss 4.748\n",
      "Ep 8 (Step 002470): Train loss 2.741, Val loss 4.751\n",
      "Ep 8 (Step 002475): Train loss 2.937, Val loss 4.752\n",
      "Ep 8 (Step 002480): Train loss 2.866, Val loss 4.715\n",
      "Ep 8 (Step 002485): Train loss 2.791, Val loss 4.723\n",
      "Ep 8 (Step 002490): Train loss 2.847, Val loss 4.728\n",
      "Ep 8 (Step 002495): Train loss 3.011, Val loss 4.735\n",
      "Ep 8 (Step 002500): Train loss 2.876, Val loss 4.705\n",
      "Ep 8 (Step 002505): Train loss 2.798, Val loss 4.684\n",
      "Ep 8 (Step 002510): Train loss 2.840, Val loss 4.711\n",
      "Ep 8 (Step 002515): Train loss 2.804, Val loss 4.731\n",
      "Every effort moves you.       \"I have been to me to me that I have been in my      that I have been heard a      that I have been at me to see that I have been  \n",
      "Ep 9 (Step 002520): Train loss 2.850, Val loss 4.713\n",
      "Ep 9 (Step 002525): Train loss 2.865, Val loss 4.761\n",
      "Ep 9 (Step 002530): Train loss 2.912, Val loss 4.802\n",
      "Ep 9 (Step 002535): Train loss 2.944, Val loss 4.794\n",
      "Ep 9 (Step 002540): Train loss 2.735, Val loss 4.796\n",
      "Ep 9 (Step 002545): Train loss 2.978, Val loss 4.825\n",
      "Ep 9 (Step 002550): Train loss 2.677, Val loss 4.785\n",
      "Ep 9 (Step 002555): Train loss 2.848, Val loss 4.792\n",
      "Ep 9 (Step 002560): Train loss 2.777, Val loss 4.801\n",
      "Ep 9 (Step 002565): Train loss 2.984, Val loss 4.808\n",
      "Ep 9 (Step 002570): Train loss 2.766, Val loss 4.798\n",
      "Ep 9 (Step 002575): Train loss 2.879, Val loss 4.816\n",
      "Ep 9 (Step 002580): Train loss 2.779, Val loss 4.852\n",
      "Ep 9 (Step 002585): Train loss 2.809, Val loss 4.853\n",
      "Ep 9 (Step 002590): Train loss 2.840, Val loss 4.843\n",
      "Ep 9 (Step 002595): Train loss 2.588, Val loss 4.845\n",
      "Ep 9 (Step 002600): Train loss 2.838, Val loss 4.847\n",
      "Ep 9 (Step 002605): Train loss 2.918, Val loss 4.863\n",
      "Ep 9 (Step 002610): Train loss 2.852, Val loss 4.844\n",
      "Ep 9 (Step 002615): Train loss 2.677, Val loss 4.826\n",
      "Ep 9 (Step 002620): Train loss 2.767, Val loss 4.832\n",
      "Ep 9 (Step 002625): Train loss 2.892, Val loss 4.816\n",
      "Ep 9 (Step 002630): Train loss 2.684, Val loss 4.808\n",
      "Ep 9 (Step 002635): Train loss 2.615, Val loss 4.807\n",
      "Ep 9 (Step 002640): Train loss 2.658, Val loss 4.822\n",
      "Ep 9 (Step 002645): Train loss 2.792, Val loss 4.817\n",
      "Ep 9 (Step 002650): Train loss 2.756, Val loss 4.815\n",
      "Ep 9 (Step 002655): Train loss 2.900, Val loss 4.826\n",
      "Ep 9 (Step 002660): Train loss 2.695, Val loss 4.866\n",
      "Ep 9 (Step 002665): Train loss 2.689, Val loss 4.833\n",
      "Ep 9 (Step 002670): Train loss 2.739, Val loss 4.812\n",
      "Ep 9 (Step 002675): Train loss 2.575, Val loss 4.830\n",
      "Ep 9 (Step 002680): Train loss 2.637, Val loss 4.820\n",
      "Ep 9 (Step 002685): Train loss 2.687, Val loss 4.779\n",
      "Ep 9 (Step 002690): Train loss 2.888, Val loss 4.790\n",
      "Ep 9 (Step 002695): Train loss 2.815, Val loss 4.752\n",
      "Ep 9 (Step 002700): Train loss 2.691, Val loss 4.743\n",
      "Ep 9 (Step 002705): Train loss 2.691, Val loss 4.788\n",
      "Ep 9 (Step 002710): Train loss 2.826, Val loss 4.803\n",
      "Ep 9 (Step 002715): Train loss 2.654, Val loss 4.805\n",
      "Ep 9 (Step 002720): Train loss 2.699, Val loss 4.794\n",
      "Ep 9 (Step 002725): Train loss 2.648, Val loss 4.781\n",
      "Ep 9 (Step 002730): Train loss 2.662, Val loss 4.790\n",
      "Ep 9 (Step 002735): Train loss 2.586, Val loss 4.777\n",
      "Ep 9 (Step 002740): Train loss 2.743, Val loss 4.789\n",
      "Ep 9 (Step 002745): Train loss 2.702, Val loss 4.795\n",
      "Ep 9 (Step 002750): Train loss 2.615, Val loss 4.762\n",
      "Ep 9 (Step 002755): Train loss 2.663, Val loss 4.763\n",
      "Ep 9 (Step 002760): Train loss 2.705, Val loss 4.792\n",
      "Ep 9 (Step 002765): Train loss 2.608, Val loss 4.812\n",
      "Ep 9 (Step 002770): Train loss 2.663, Val loss 4.820\n",
      "Ep 9 (Step 002775): Train loss 2.786, Val loss 4.794\n",
      "Ep 9 (Step 002780): Train loss 2.749, Val loss 4.808\n",
      "Ep 9 (Step 002785): Train loss 2.867, Val loss 4.802\n",
      "Ep 9 (Step 002790): Train loss 2.830, Val loss 4.781\n",
      "Ep 9 (Step 002795): Train loss 2.982, Val loss 4.792\n",
      "Ep 9 (Step 002800): Train loss 2.741, Val loss 4.790\n",
      "Ep 9 (Step 002805): Train loss 2.683, Val loss 4.765\n",
      "Ep 9 (Step 002810): Train loss 2.755, Val loss 4.745\n",
      "Ep 9 (Step 002815): Train loss 2.752, Val loss 4.770\n",
      "Ep 9 (Step 002820): Train loss 2.697, Val loss 4.788\n",
      "Ep 9 (Step 002825): Train loss 2.599, Val loss 4.792\n",
      "Ep 9 (Step 002830): Train loss 2.709, Val loss 4.787\n",
      "Every effort moves you.       \"I have not have no doubt you, I have been in the      you, I have been a      you, and I have to me, and I have you, and\n",
      "Ep 10 (Step 002835): Train loss 2.678, Val loss 4.770\n",
      "Ep 10 (Step 002840): Train loss 2.767, Val loss 4.789\n",
      "Ep 10 (Step 002845): Train loss 2.603, Val loss 4.800\n",
      "Ep 10 (Step 002850): Train loss 2.733, Val loss 4.837\n",
      "Ep 10 (Step 002855): Train loss 2.691, Val loss 4.880\n",
      "Ep 10 (Step 002860): Train loss 2.739, Val loss 4.872\n",
      "Ep 10 (Step 002865): Train loss 2.730, Val loss 4.866\n",
      "Ep 10 (Step 002870): Train loss 2.723, Val loss 4.878\n",
      "Ep 10 (Step 002875): Train loss 2.717, Val loss 4.886\n",
      "Ep 10 (Step 002880): Train loss 2.869, Val loss 4.916\n",
      "Ep 10 (Step 002885): Train loss 2.807, Val loss 4.893\n",
      "Ep 10 (Step 002890): Train loss 2.755, Val loss 4.871\n",
      "Ep 10 (Step 002895): Train loss 2.756, Val loss 4.886\n",
      "Ep 10 (Step 002900): Train loss 2.638, Val loss 4.895\n",
      "Ep 10 (Step 002905): Train loss 2.678, Val loss 4.921\n",
      "Ep 10 (Step 002910): Train loss 2.682, Val loss 4.889\n",
      "Ep 10 (Step 002915): Train loss 2.668, Val loss 4.894\n",
      "Ep 10 (Step 002920): Train loss 2.692, Val loss 4.901\n",
      "Ep 10 (Step 002925): Train loss 2.649, Val loss 4.920\n",
      "Ep 10 (Step 002930): Train loss 2.649, Val loss 4.902\n",
      "Ep 10 (Step 002935): Train loss 2.873, Val loss 4.913\n",
      "Ep 10 (Step 002940): Train loss 2.769, Val loss 4.915\n",
      "Ep 10 (Step 002945): Train loss 2.644, Val loss 4.917\n",
      "Ep 10 (Step 002950): Train loss 2.405, Val loss 4.916\n",
      "Ep 10 (Step 002955): Train loss 2.740, Val loss 4.911\n",
      "Ep 10 (Step 002960): Train loss 2.358, Val loss 4.912\n",
      "Ep 10 (Step 002965): Train loss 2.837, Val loss 4.914\n",
      "Ep 10 (Step 002970): Train loss 2.810, Val loss 4.887\n",
      "Ep 10 (Step 002975): Train loss 2.786, Val loss 4.895\n",
      "Ep 10 (Step 002980): Train loss 2.744, Val loss 4.918\n",
      "Ep 10 (Step 002985): Train loss 2.732, Val loss 4.892\n",
      "Ep 10 (Step 002990): Train loss 2.740, Val loss 4.885\n",
      "Ep 10 (Step 002995): Train loss 2.738, Val loss 4.906\n",
      "Ep 10 (Step 003000): Train loss 2.595, Val loss 4.897\n",
      "Ep 10 (Step 003005): Train loss 2.750, Val loss 4.854\n",
      "Ep 10 (Step 003010): Train loss 2.677, Val loss 4.873\n",
      "Ep 10 (Step 003015): Train loss 2.894, Val loss 4.896\n",
      "Ep 10 (Step 003020): Train loss 2.766, Val loss 4.880\n",
      "Ep 10 (Step 003025): Train loss 2.655, Val loss 4.869\n",
      "Ep 10 (Step 003030): Train loss 2.335, Val loss 4.878\n",
      "Ep 10 (Step 003035): Train loss 2.510, Val loss 4.873\n",
      "Ep 10 (Step 003040): Train loss 2.659, Val loss 4.869\n",
      "Ep 10 (Step 003045): Train loss 2.599, Val loss 4.845\n",
      "Ep 10 (Step 003050): Train loss 2.525, Val loss 4.871\n",
      "Ep 10 (Step 003055): Train loss 2.630, Val loss 4.854\n",
      "Ep 10 (Step 003060): Train loss 2.640, Val loss 4.848\n",
      "Ep 10 (Step 003065): Train loss 2.645, Val loss 4.844\n",
      "Ep 10 (Step 003070): Train loss 2.471, Val loss 4.844\n",
      "Ep 10 (Step 003075): Train loss 2.613, Val loss 4.869\n",
      "Ep 10 (Step 003080): Train loss 2.734, Val loss 4.900\n",
      "Ep 10 (Step 003085): Train loss 2.375, Val loss 4.874\n",
      "Ep 10 (Step 003090): Train loss 2.629, Val loss 4.865\n",
      "Ep 10 (Step 003095): Train loss 2.642, Val loss 4.867\n",
      "Ep 10 (Step 003100): Train loss 2.555, Val loss 4.869\n",
      "Ep 10 (Step 003105): Train loss 2.566, Val loss 4.880\n",
      "Ep 10 (Step 003110): Train loss 2.378, Val loss 4.872\n",
      "Ep 10 (Step 003115): Train loss 2.584, Val loss 4.879\n",
      "Ep 10 (Step 003120): Train loss 2.543, Val loss 4.875\n",
      "Ep 10 (Step 003125): Train loss 2.687, Val loss 4.895\n",
      "Ep 10 (Step 003130): Train loss 2.448, Val loss 4.904\n",
      "Ep 10 (Step 003135): Train loss 2.589, Val loss 4.891\n",
      "Ep 10 (Step 003140): Train loss 2.367, Val loss 4.882\n",
      "Ep 10 (Step 003145): Train loss 2.487, Val loss 4.875\n",
      "Every effort moves you.       \"I have no more. I have no more. I have seen      you. I have seen the      that you have no doubt that I have seen the    \n",
      "Training completed in 10.01 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.01)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=\"Every effort moves you\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "af3e11b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    \n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # Only shows integer labels on x-axis\n",
    "    \n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()   # Creates a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)    # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "51dc757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Both the training and validation losses start to improve for the first epoch. However, the losses start to diverge past the\n",
    "### second epoch.\n",
    "### This divergence and the fact that the validation loss is much larger than the training loss indicates that the model is \n",
    "### overfitting to the training data.\n",
    "### We can confirm that the model memorizes the training data verbatim by searching for the generated text snippets, such as \n",
    "### \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\" in the text file.\n",
    "### This memorization is expected since we are working with a very small training dataset and training the model for multiple epochs.\n",
    "### Usually, it is common to train a model on a much much larger dataset for only one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5159a8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
