{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9eccd0f",
   "metadata": {},
   "source": [
    "### Generating text from output tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f20c36e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: idx is a (batch, n_tokens) array of indices in the current context\n",
    "### Step 2: Crop current context if it exceeds the supported context size ex: if LLM supports only 5 tokens, and the context size\n",
    "### is 10 then only the last 5 tokens are used as context.\n",
    "### Step 3: Focus only on the last time step, so that (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "### Step 4: probas has shape (batch, vocab_size)\n",
    "### Step 5: idx_next has shape (batch, 1)\n",
    "### Step 6: Append sampled index to the running sequence, where idx has shape (batch, n_tokens + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "912156ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70baf67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceed the supported context size\n",
    "        # ex: if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "            \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, voacb_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "        \n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)   # (batch, 1)\n",
    "        \n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)    # (batch, n_tokens + 1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dacac029",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The softmax function is monotonic, meaning it preserves the order of its inputs when transformed into outputs.\n",
    "### So, in practice, the softmax step is redundant since the position with the highest score in the softmax output tensor\n",
    "### is the same position in the logit tensor.\n",
    "### In other words, we could apply the torch.argmax function to the logits tensor directly and get identical results.\n",
    "### However, we coded the conversion to illustrate the full process of transforming logits to probabilities, which can add\n",
    "### additional intuition, such as that the model generates the most likely next token, which is known as greedy decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf42888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
